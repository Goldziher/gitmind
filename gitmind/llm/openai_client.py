from collections.abc import Generator
from typing import TYPE_CHECKING, Any, Union, cast

from tree_sitter_language_pack import SupportedLanguage, get_binding

from gitmind.config import AzureOpenAIProviderConfig, OpenAIProviderConfig
from gitmind.exceptions import EmptyContentError, LLMClientError, MissingDependencyError
from gitmind.llm.base import LLMClient, MessageDefinition, MessageRole, ToolDefinition
from gitmind.utils.chunking import ChunkingType, get_chunker

try:
    from openai import NOT_GIVEN, OpenAIError
    from openai.types import ChatModel
    from openai.types.chat import (
        ChatCompletionMessageParam,
        ChatCompletionSystemMessageParam,
        ChatCompletionToolParam,
        ChatCompletionUserMessageParam,
    )
    from openai.types.chat.completion_create_params import ResponseFormat
    from openai.types.shared_params import FunctionDefinition

    if TYPE_CHECKING:
        from openai import AsyncClient
        from openai.lib.azure import AsyncAzureOpenAI
except ImportError as e:
    raise MissingDependencyError("openai is not installed") from e

__all__ = ["OpenAIClient"]

_openai_message_mapping: dict[MessageRole, type[ChatCompletionMessageParam]] = {
    "system": ChatCompletionSystemMessageParam,
    "user": ChatCompletionUserMessageParam,
}


class OpenAIClient(LLMClient[AzureOpenAIProviderConfig | OpenAIProviderConfig]):
    """Wrapper for OpenAI models."""

    _client: Union["AsyncAzureOpenAI", "AsyncClient"]
    """The OpenAI client instance."""
    _model: ChatModel | str
    """The model to use for generating completions."""

    __slots__ = ("_client", "_model")

    def __init__(self, *, config: AzureOpenAIProviderConfig | OpenAIProviderConfig) -> None:
        """Initialize the OpenAI Client.

        Args:
            config: The OpenAI or AzureOpenAI provider config.
        """
        if isinstance(config, AzureOpenAIProviderConfig):
            from openai.lib.azure import AsyncAzureOpenAI

            self._client = AsyncAzureOpenAI(
                azure_endpoint=config.endpoint,
                azure_deployment=config.deployment,
                api_version=config.api_version,
                api_key=config.api_key,
                azure_ad_token=config.ad_token,
                max_retries=config.max_retries,
            )
        else:
            from openai import AsyncClient

            self._client = AsyncClient(
                api_key=config.api_key,
                base_url=config.base_url,
                max_retries=config.max_retries,
            )
            self._model = config.model

    async def create_completions(
        self,
        *,
        messages: list[MessageDefinition],
        json_response: bool = False,
        tool: ToolDefinition | None = None,
        **kwargs: Any,
    ) -> str:
        """Create completions.

        Args:
            messages: The messages to generate completions for.
            json_response: Whether to return the response as a JSON object.
            tool: An optional tool call.
            **kwargs: Additional completion options.

        Raises:
            LLMClientError: If an error occurs while creating completions.
            EmptyContentError: If the LLM client returns empty content.

        Returns:
            The completion generated by the client.
        """
        try:
            result = await self._client.chat.completions.create(  # type: ignore[call-overload]
                model=self._model,
                messages=[
                    _openai_message_mapping[message.role](role=message.role, content=message.content)  # type: ignore[call-arg,arg-type]
                    for message in messages
                ],
                response_format=ResponseFormat(type="json_object" if json_response else "text"),
                stream=False,
                tools=[
                    ChatCompletionToolParam(
                        type="function",
                        function=FunctionDefinition(
                            name=tool.name,
                            parameters=tool.parameters,
                            description=tool.description or "",
                        ),
                    )
                ]
                if tool is not None
                else NOT_GIVEN,
                tool_choice="required" if tool else NOT_GIVEN,
                **kwargs,
            )
        except OpenAIError as e:
            raise LLMClientError("Failed to generate completion", context=str(e)) from e

        if content := result.choices[0].message.tool_calls[0].function.arguments:
            return cast(str, content)

        raise EmptyContentError("LLM client returned empty content", context=result.model_dump_json())

    def chunk_content(
        self, content: str, max_tokens: int, chunking_type: ChunkingType, language: SupportedLanguage | None = None
    ) -> Generator[str, None, None]:
        """Chunk the given content into chunks of the given size.

        Args:
            content: The content to chunk.
            max_tokens: The maximum number of tokens per chunk.
            chunking_type: The type of content to chunk.
            language: The language to use for code chunking.

        Returns:
            A list of chunks.
        """
        kwargs = {"model": self._model, "capacity": max_tokens}
        if language:
            kwargs["language"] = get_binding(language)

        chunker = get_chunker(chunking_type=chunking_type, language=language, chunk_size=max_tokens, model=self._model)  # type: ignore[arg-type]
        yield from chunker.chunks(content)
