from __future__ import annotations

from typing import TYPE_CHECKING, Any, cast

from gitmind.exceptions import EmptyContentError, LLMClientError, MissingDependencyError
from gitmind.llm.base import LLMClient, MessageDefinition, MessageRole, ToolDefinition

try:
    from groq import NOT_GIVEN, GroqError
    from groq.types.chat import (
        ChatCompletionMessageParam,
        ChatCompletionSystemMessageParam,
        ChatCompletionToolParam,
        ChatCompletionUserMessageParam,
    )
    from groq.types.chat.completion_create_params import ResponseFormat
    from groq.types.shared_params import FunctionDefinition

    if TYPE_CHECKING:
        from groq import AsyncClient
except ImportError as e:
    raise MissingDependencyError("groq is not installed") from e

__all__ = ["GroqClient"]

_groq_message_mapping: dict[MessageRole, type[ChatCompletionMessageParam]] = {
    "system": ChatCompletionSystemMessageParam,
    "user": ChatCompletionUserMessageParam,
}


class GroqClient(LLMClient):
    """Groq LLM client.

    Args:
        api_key: The API key for the provider.
        model_name: The model to use for completions.
        endpoint_url: The endpoint URL for the provider.
        **kwargs: Additional client options.
    """

    _client: AsyncClient
    """The Groq client instance."""
    _model: str
    """The model to use for generating completions."""

    __slots__ = ("_client", "_model")

    def __init__(
        self,
        *,
        api_key: str,
        model_name: str,
        endpoint_url: str | None = None,
        **kwargs: Any,
    ) -> None:
        from groq import AsyncClient

        self._client = AsyncClient(
            api_key=api_key,
            base_url=endpoint_url,
            **kwargs,
        )
        self._model = model_name

    async def create_completions(
        self,
        *,
        messages: list[MessageDefinition],
        json_response: bool = False,
        tool: ToolDefinition | None = None,
        **kwargs: Any,
    ) -> str:
        """Create completions.

        Args:
            messages: The messages to generate completions for.
            json_response: Whether to return the response as a JSON object.
            tool: An optional tool call.
            **kwargs: Additional completion options.

        Raises:
            LLMClientError: If an error occurs while creating completions.

        Returns:
            The completion generated by the client.
        """
        try:
            result = await self._client.chat.completions.create(  # type: ignore[call-overload]
                model=self._model,
                messages=[
                    _groq_message_mapping[message.role](role=message.role, content=message.content)  # type: ignore[call-arg,arg-type]
                    for message in messages
                ],
                response_format=ResponseFormat(type="json_object" if json_response else "text"),
                stream=not tool,
                tools=[
                    ChatCompletionToolParam(
                        type="function",
                        function=FunctionDefinition(
                            name=tool.name,
                            parameters=tool.parameters,
                            description=tool.description or "",
                        ),
                    )
                ]
                if tool is not None
                else NOT_GIVEN,
                tool_choice="auto" if tool else NOT_GIVEN,
                **kwargs,
            )
        except GroqError as e:
            raise LLMClientError("Failed to generate completion", context=str(e)) from e

        if content := result.choices[0].message.tool_calls[0].function.arguments:
            return cast(str, content)

        raise EmptyContentError("LLM client returned empty content", context=result.model_dump_json())
