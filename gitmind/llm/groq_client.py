from __future__ import annotations

from typing import TYPE_CHECKING, Any, Callable, cast

from gitmind.exceptions import EmptyContentError, LLMClientError, MissingDependencyError
from gitmind.llm.base import LLMClient, MessageDefinition, MessageRole, ToolDefinition

try:
    from groq import NOT_GIVEN, GroqError
    from groq.types.chat import (
        ChatCompletionAssistantMessageParam,
        ChatCompletionMessageParam,
        ChatCompletionSystemMessageParam,
        ChatCompletionToolMessageParam,
        ChatCompletionToolParam,
        ChatCompletionUserMessageParam,
    )
    from groq.types.chat.completion_create_params import ResponseFormat
    from groq.types.shared_params import FunctionDefinition

    if TYPE_CHECKING:
        from groq import AsyncClient
except ImportError as e:
    raise MissingDependencyError("groq is not installed") from e

__all__ = ["GroqClient"]


_groq_message_mapping = cast(
    "dict[MessageRole, Callable[..., ChatCompletionMessageParam]]",
    {
        "system": lambda role, content: ChatCompletionSystemMessageParam(role=role, content=content),
        "user": lambda role, content: ChatCompletionUserMessageParam(role=role, content=content),
        "tool": lambda role, content: ChatCompletionToolMessageParam(role=role, content=content, tool_call_id=""),
        "assistant": lambda role, content: ChatCompletionAssistantMessageParam(role=role, content=content),
    },
)


class GroqClient(LLMClient):
    """Groq LLM client.

    Args:
        api_key: The API key for the provider.
        model_name: The model to use for completions.
        endpoint_url: The endpoint URL for the provider.
        **kwargs: Additional client options.
    """

    _client: AsyncClient
    """The Groq client instance."""
    _model: str
    """The model to use for generating completions."""

    __slots__ = ("_client", "_model")

    def __init__(
        self,
        *,
        api_key: str,
        model_name: str,
        endpoint_url: str | None = None,
        **kwargs: Any,
    ) -> None:
        from groq import AsyncClient

        self._client = AsyncClient(
            api_key=api_key,
            base_url=endpoint_url,
            **kwargs,
        )
        self._model = model_name

    async def create_completions(
        self,
        *,
        messages: list[MessageDefinition],
        json_response: bool = False,
        tool: ToolDefinition | None = None,
        **kwargs: Any,
    ) -> str:
        """Create completions.

        Args:
            messages: The messages to generate completions for.
            json_response: Whether to return the response as a JSON object.
            tool: An optional tool call.
            **kwargs: Additional completion options.

        Raises:
            LLMClientError: If an error occurs while creating completions.
            EmptyContentError: If the LLM client returns empty content.

        Returns:
            The completion generated by the client.
        """
        try:
            result = await self._client.chat.completions.create(
                model=self._model,
                messages=[
                    _groq_message_mapping[message.role](role=message.role, content=message.content)
                    for message in messages
                ],
                response_format=ResponseFormat(type="json_object" if json_response else "text"),
                stream=not tool,
                tools=[
                    ChatCompletionToolParam(
                        type="function",
                        function=FunctionDefinition(
                            name=tool.name,
                            parameters=tool.parameters,
                            description=tool.description or "",
                        ),
                    )
                ]
                if tool is not None
                else NOT_GIVEN,
                tool_choice="auto" if tool else NOT_GIVEN,
                **kwargs,
            )
        except GroqError as e:
            raise LLMClientError("Failed to generate completion", context=str(e)) from e

        if hasattr(result, "choices"):
            if (
                result.choices
                and result.choices[0].message.tool_calls
                and result.choices[0].message.tool_calls[0].function.arguments
            ):
                return cast("str", result.choices[0].message.tool_calls[0].function.arguments)

            raise EmptyContentError("LLM client returned empty content", context=result.model_dump_json())  # type: ignore[union-attr]

        raise EmptyContentError("Unexpected streaming response when using tool")
