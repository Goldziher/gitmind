from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Final, Generic, TypeVar

from anyio import sleep
from jsonschema import ValidationError, validate
from msgspec import DecodeError

from gitmind.exceptions import LLMClientError
from gitmind.llm.base import LLMClient, MessageDefinition, RetryConfig, ToolDefinition
from gitmind.utils.logger import get_logger
from gitmind.utils.serialization import deserialize, serialize

logger = get_logger(__name__)

T = TypeVar("T")
R = TypeVar("R")


MAX_TOKENS: Final[int] = 4096

VALIDATION_ERROR_MESSAGE_CONTENT: Final[str] = """
Your previous response failed validation. Here is the error:\n{e}\nHere is your previous response:\n{response}\n
Generate a new response that fixes the validation error and completely satisfies the tool parameters.
"""


class AbstractPromptHandler(ABC, Generic[T]):
    """Base class for LLM prompt handlers.

    Args:
            client: The LLM client to use.
            retry_config: The retry configuration to use.
            max_response_tokens: The maximum number of tokens in the response.
    """

    __slots__ = ("_client", "_retry_config", "_chunk_size", "_max_response_tokens")

    def __init__(
        self,
        client: LLMClient,
        retry_config: RetryConfig | None = None,
        max_response_tokens: int | None = None,
    ) -> None:
        self._client = client
        self._retry_config = retry_config if retry_config else RetryConfig()
        self._max_response_tokens = max_response_tokens if max_response_tokens else MAX_TOKENS

    @abstractmethod
    async def __call__(self, **kwargs: Any) -> T:
        """Generate LLM completions.

        Args:
            **kwargs: Additional arguments .

        Returns:
            The completion generated by the client.
        """
        ...

    async def generate_completions(
        self,
        *,
        messages: list[MessageDefinition],
        response_type: type[R],
        retry_count: int = 0,
        schema: dict[str, Any],
        tool: ToolDefinition | None = None,
    ) -> R:
        """Generate LLM completions.

        Args:
            messages: The messages to generate completions for.
            response_type: The type of the response.
            retry_count: The number of retries attempted.
            schema: The schema to use for the completions.
            tool: An optional tool call.

        Raises:
            LLMClientError: If an error occurs while generating completions.

        Returns:
            The response from the LLM client.
        """
        try:
            logger.debug(
                "%s: Generating completions.\n\nPrompt: %s", self.__class__.__name__, serialize(messages).decode()
            )
            response = await self._client.create_completions(
                messages=messages, json_response=True, tool=tool, max_tokens=self._max_response_tokens
            )
        except LLMClientError as e:
            logger.error("%s: Error occurred while generating completions: %s.", self.__class__.__name__, e)
            raise
        try:
            logger.debug("%s: Successfully generated completions.\n\nResponse: %s", self.__class__.__name__, response)
            result = deserialize(response, response_type)
            validate(instance=result, schema=schema)
            return result
        except (DecodeError, ValidationError) as e:
            # This has to be in place because LLMs sometimes return invalid or partial JSON
            validation_error_message = MessageDefinition(
                role="user", content=VALIDATION_ERROR_MESSAGE_CONTENT.format(e=str(e), response=response)
            )

            if retry_count == 0:
                messages.append(validation_error_message)
            else:
                messages[-1] = validation_error_message

            if retry_count < self._retry_config.max_retries:
                retry_count += 1
                logger.warning(
                    "Validation failed.\nError: %s,\n\nRetrying (%d/%d)",
                    e,
                    retry_count,
                    self._retry_config.max_retries,
                )
                await sleep((2**retry_count) if self._retry_config.exponential_backoff else 1)
                return await self.generate_completions(
                    messages=messages,
                    response_type=response_type,
                    retry_count=retry_count,
                    schema=schema,
                    tool=tool,
                )
            logger.warning("LLM responded with invalid or partial JSON response, retries have been exhausted.")
            raise LLMClientError(
                "LLM responded with invalid or partial JSON response",
                context=str(e),
            ) from e
