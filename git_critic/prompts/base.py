from abc import ABC, abstractmethod
from typing import Any, Generic, TypeVar

from anyio import sleep
from jsonschema import ValidationError, validate
from msgspec import DecodeError

from git_critic.configuration_types import RetryConfig
from git_critic.exceptions import LLMClientError
from git_critic.llm.base import LLMClient
from git_critic.utils.logger import get_logger
from git_critic.utils.serialization import deserialize

logger = get_logger(__name__)

T = TypeVar("T")


class AbstractPromptHandler(ABC, Generic[T]):
    """Base class for LLM prompt handlers."""

    __slots__ = ("_client", "_retry_config")

    def __init__(self, client: LLMClient, retry_config: RetryConfig | None = None):
        """Initialize the prompt handler.

        Args:
            client: The LLM client to use.
            retry_config: The retry configuration to use.
        """
        self._client = client
        self._retry_config = retry_config if retry_config else RetryConfig()

    @abstractmethod
    async def __call__(self, **kwargs: Any) -> T:
        """Generate LLM completions.

        Args:
            **kwargs: Additional arguments .

        Returns:
            The completion generated by the client.
        """
        raise NotImplementedError("Method not implemented")

    async def generate_completions(
        self,
        *,
        properties: dict[str, Any],
        retry_count: int = 0,
        **kwargs: Any,
    ) -> dict[str, Any]:
        """Generate LLM completions.


        Args:
            properties: The properties to validate.
            retry_count: The number of retries attempted.
            **kwargs: Additional arguments to pass to the LLM client.

        Returns:
            The response from the LLM client.
        """
        try:
            logger.debug("%s: Generating completions.", self.__class__.__name__)
            response = await self._client.create_completions(**kwargs)
            logger.debug("%s: Successfully generated completions.", self.__class__.__name__)
            return self.parse_json_response(response=response, properties=properties)
        except LLMClientError as e:
            logger.error("%s: Error occurred while generating completions: %s.", self.__class__.__name__, e)
            raise
        except (DecodeError, ValidationError) as e:
            # This has to be in place because LLMs sometimes return invalid or partial JSON
            if retry_count < self._retry_config.max_retries:
                retry_count += 1
                logger.warning(
                    "LLM responded with an invalid or partial JSON response, retrying (%d/%d)",
                    retry_count,
                    self._retry_config.max_retries,
                )
                await sleep((2**retry_count) if self._retry_config.exponential_backoff else 1)
                return await self.generate_completions(
                    properties=properties,
                    retry_count=retry_count,
                    **kwargs,
                )
            logger.warning("LLM responded with invalid or partial JSON response, retries have been exhausted.")
            raise LLMClientError(
                "LLM responded with invalid or partial JSON response",
                context=str(e),
            ) from e

    @staticmethod
    def parse_json_response(*, response: str, properties: dict[str, Any]) -> dict[str, Any]:
        """Validate the response from the LLM client.

        Args:
            response: The response from the LLM client.
            properties: The properties to validate.

        Returns:
            The validated response.
        """
        parsed_response = {k: v for k, v in deserialize(response, dict).items() if k in properties}

        validate(
            instance=parsed_response,
            schema={
                "$schema": "http://json-schema.org/draft-07/schema#",
                "type": "object",
                "properties": properties,
                "additionalProperties": False,
                "required": list(properties),
            },
        )

        return parsed_response
